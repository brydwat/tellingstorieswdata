---
title: "tswd_ch8_notes"
author: "Bryce Watson"
date: "2025-01-25"
output:
  html_document:
    theme:
      bootswatch: cerulean
---
# Experiments and Surveys
### Causal Inference overview

The effect of a treatment can be assumed to be causal if (Yi|t=0) =/= (Yi|t=1) and groups are reliably equal.

Individual counterfactual is impossible, so must be checked between treatment and control group averages.

Recall Bastiat, what is seen and what is unseen. The counterfactual will always remain unseen in the individual.

Methods such as Propensity Score Matching used to match groups and minimize differences.

```{r, warning=FALSE}

library(haven)
library(labelled)
library(tidyverse)
library(tinytable)

set.seed(853)

treat_control <-
  tibble(
    group = sample(x = c("Treatment", "Control"), size = 100, replace = TRUE),
    binary_effect = sample(x = c(0, 1), size = 100, replace = TRUE)
  )

treat_control |>
  summarise(
    treat_result = sum(binary_effect) / length(binary_effect),
    .by = group
  )
```

### Randomization
Ideally random selection of two groups should yield two groups that are fundamentally the same  except for the treatment given. 

RCTs and A/B testing are gold standards, but not always the best methods to use. 

### Simulated example: Cats or dogs.
Simulate a population.
Determine if people who own cats and dogs tend to prefer blue or white.
```{r, warning=FALSE}
num_people <- 5000

population <- tibble(
  person = 1:num_people,
  favorite_color = sample(c("Blue","White"), size = num_people, replace = TRUE),
  prefers_dogs = if_else(favorite_color == "Blue",
                         rbinom(num_people, 1, 0.9),
                         rbinom(num_people, 1, 0.1))
)

population |>
  count(favorite_color, prefers_dogs)


```

Now construct a sampling frame.
```{r, warning=FALSE}
set.seed(853)

frame <-
  population |>
  mutate(in_frame = rbinom(n = num_people, 1, prob = 0.8))|>
  filter(in_frame == 1)

frame |>
  count(favorite_color, prefers_dogs)
```

Create treatment and control groups with favorite color only.
```{r, warning=FALSE}
set.seed(853)

sample <-
  frame |>
  select(-prefers_dogs) |>
  mutate(
    group = 
      sample(x = c("Treatment", "Control"), size = nrow(frame), replace = TRUE
  ))
```

Count the groups and calculate proportions.
```{r, warning=FALSE}
sample |>
  count(group, favorite_color) |>
  mutate(prop = n / sum(n),
         .by = group) |>
  tt() |> 
  style_tt(j = 1:4, align = "llrr") |> 
  format_tt(digits = 2, num_mark_big = ",", num_fmt = "decimal") |> 
  setNames(c("Group", "Prefers", "Number", "Proportion"))
```

Analysis of Variance (ANOVA)
 Introduced by Fisher.
 Modern use cases are few. Will expand in ch 12.
 
### Treatment and Control
Internal validity - if the treatment and control groups are the same in all ways and remain that way, except for the treatment. 

External Validity - If the group which was randomized into treatment and control is representative of the larger population, and the experiment replicates outside conditions. 

But this means we need randomization twice. Firstly, into the group that was subject to the experiment (for external validity), and then secondly, between treatment and control (for internal validity).

If the treatment is truly independent - estimate the Average Treatment Effect (ATE) in a binary treatment variable. 

ATE = E[Y|t = 1] - E[Y|t = 0]
That is, the difference between the treated group,  t = 1, and the control group, t = 0, when measured by the expected value of the outcome, Y. The ATE becomes the difference between two conditional expectations.

Practical questions to explore when working with real experimental data.

1. How are participants being selected?
2. How are they being slected for treatment?
  - Early success may lead to pressure to treat everyone. 
  
3. How is treatment being assessed?
4. Is random allocation ethical and fair in the experiment? 
5. Is selection bias recognized and adjusted for?

We are concerned with “equipoise”, by which we refer to a situation in which there is genuine uncertainty about whether the treatment is more effective than conventional procedures. In medical settings even if there is initial equipoise it could be undermined if the treatment is found to be effective early in the study.Berry (1989) points out that there is almost never complete consensus and so one could almost always argue, inappropriately, for the existence of equipoise even in the face of a substantial weight of evidence.

# Surveys

When doing surveys, it is critical to ask the right person. For instance, Lichand and Wolf (2022) consider child labor. The extent of child labor is typically based on surveys of parents. When children were surveyed a considerable under-reporting by parents was found.


# RCT Examples

### Oregon Health Insurance experiment

10,000 participants assigned by random lottery.

The decision to buy health insurance is usually confounded, so random allocation was important.

Like earlier studies such as Brook et al. (1984), Finkelstein et al. (2012) found that the treatment group used more health care including both primary and preventive care as well as hospitalizations but had lower out-of-pocket medical expenditures. More generally, the treatment group reported better physical and mental health.

# A/B Testing

More experiments completed in the last decade than in all previous decades due to extensive use of A/B testing.
 - is there any broader epistemic gain of this?
 
Many large tech companies have extensive infrastructure for experimentation.

Private sector has different ethical constraints than academia. End is to drive more income/profit. Potentially not an issue with grocery firms, but could be an issue with gambling firms.

A/B testing refers to experiments primarily implemented through technology stack about something primarily online, such as UI interaction, ad placement, change to a website, etc. 

Statistical methods may be slightly different due to volume of A/B tests vs RCT experiments which usually take months for a single experiment - Bosch and Revilla 2022.


Three concerns for A/B testing
 1. Delivery decisions - different platforms my have different users, or bias the data in subtle ways.
 
 2. Instrumentation - A/B testing is usually not done with surveys, but with sensors. Behavior in relation to these sensors (i.e. clearing cookies) is important to understand.
 
 3. What is being randomized over? In RCTs it is by subject. In A/B testing it can be unclear.
 
 
Code for Quiz Question 11
```{r, warning=FALSE}
### Simulated Dataset ###

netflix_data <-
  tibble(
    person = c("Ian", "Ian", "Roger", "Roger",
      "Roger", "Patricia", "Patricia", "Helen"
    ),
    tv_show = c(
      "Broadchurch", "Duty-Shame", "Broadchurch", "Duty-Shame",
      "Shetland", "Broadchurch", "Shetland", "Duty-Shame"
    ),
    hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2)
  )

### My Code ###
#Randomly assign people into one of two groups.
set.seed(345)
group <- c("A","B")

randomized_data <- mutate(
  netflix_data, 
  group = sample(group, nrow(netflix_data), replace = TRUE)
)


```
